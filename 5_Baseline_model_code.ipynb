{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook, je présente l'implémentation et les résultats du modèle Baseline du projet 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modélisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import html5lib\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from string import punctuation\n",
    "import nltk.corpus\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_feather('cleaned_data_stackoverflow_questions.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43801 entries, 0 to 43800\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Title_cleaned    43801 non-null  object\n",
      " 1   Body_cleaned     43801 non-null  object\n",
      " 2   Tags_cleaned     43801 non-null  object\n",
      " 3   Title_tokenized  43801 non-null  object\n",
      " 4   Body_tokenized   43801 non-null  object\n",
      " 5   Tags_tokenized   43801 non-null  object\n",
      " 6   Tags_list        43801 non-null  object\n",
      " 7   number_of_tags   43801 non-null  int64 \n",
      " 8   Id               43801 non-null  int64 \n",
      " 9   Score            43801 non-null  int64 \n",
      " 10  ViewCount        43801 non-null  int64 \n",
      " 11  AnswerCount      43801 non-null  int64 \n",
      "dtypes: int64(5), object(7)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title_cleaned</th>\n",
       "      <th>Body_cleaned</th>\n",
       "      <th>Tags_cleaned</th>\n",
       "      <th>Title_tokenized</th>\n",
       "      <th>Body_tokenized</th>\n",
       "      <th>Tags_tokenized</th>\n",
       "      <th>Tags_list</th>\n",
       "      <th>number_of_tags</th>\n",
       "      <th>Id</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when porting java code to objc how best to rep...</td>\n",
       "      <td>i am working on porting a java codebase to coc...</td>\n",
       "      <td>java objective-c cocoa macos porting</td>\n",
       "      <td>[porting, java, code, objc, best, represent, c...</td>\n",
       "      <td>[working, porting, java, codebase, cocoaobject...</td>\n",
       "      <td>[java, objective-c, cocoa, macos, porting]</td>\n",
       "      <td>[java, objective-c, macos]</td>\n",
       "      <td>3</td>\n",
       "      <td>1117384</td>\n",
       "      <td>8</td>\n",
       "      <td>1346</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is there a way for hiding some enum values for...</td>\n",
       "      <td>i have enum lets say for example and have two ...</td>\n",
       "      <td>c# .net winforms windows-forms-designer prope...</td>\n",
       "      <td>[way, hiding, enum, values, specific, property...</td>\n",
       "      <td>[enum, lets, say, example, two, classes, prope...</td>\n",
       "      <td>[c#, .net, winforms, windows-forms-designer, p...</td>\n",
       "      <td>[c#, .net]</td>\n",
       "      <td>2</td>\n",
       "      <td>59024032</td>\n",
       "      <td>8</td>\n",
       "      <td>2172</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fixing words with spaces using a dictionary lo...</td>\n",
       "      <td>i have extracted the list of sentences from a ...</td>\n",
       "      <td>python python-2.7 dictionary nltk text-segmen...</td>\n",
       "      <td>[fixing, words, spaces, using, dictionary, loo...</td>\n",
       "      <td>[extracted, list, sentences, document, preproc...</td>\n",
       "      <td>[python, python-2.7, dictionary, nltk, text-se...</td>\n",
       "      <td>[python]</td>\n",
       "      <td>1</td>\n",
       "      <td>19675106</td>\n",
       "      <td>19</td>\n",
       "      <td>6507</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how to split a string at line breaks in python</td>\n",
       "      <td>i want to copy some tabular data from excel in...</td>\n",
       "      <td>python arrays list clipboard pywin32</td>\n",
       "      <td>[split, string, line, breaks, python]</td>\n",
       "      <td>[want, copy, tabular, data, excel, python, arr...</td>\n",
       "      <td>[python, arrays, list, clipboard, pywin32]</td>\n",
       "      <td>[python, arrays]</td>\n",
       "      <td>2</td>\n",
       "      <td>21205074</td>\n",
       "      <td>6</td>\n",
       "      <td>7848</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how do you express binary literals in python</td>\n",
       "      <td>how do you express an integer as a binary numb...</td>\n",
       "      <td>python syntax binary integer literals</td>\n",
       "      <td>[express, binary, literals, python]</td>\n",
       "      <td>[express, integer, binary, number, python, lit...</td>\n",
       "      <td>[python, syntax, binary, integer, literals]</td>\n",
       "      <td>[python]</td>\n",
       "      <td>1</td>\n",
       "      <td>1476</td>\n",
       "      <td>394</td>\n",
       "      <td>317942</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Title_cleaned  \\\n",
       "0  when porting java code to objc how best to rep...   \n",
       "1  is there a way for hiding some enum values for...   \n",
       "2  fixing words with spaces using a dictionary lo...   \n",
       "3     how to split a string at line breaks in python   \n",
       "4       how do you express binary literals in python   \n",
       "\n",
       "                                        Body_cleaned  \\\n",
       "0  i am working on porting a java codebase to coc...   \n",
       "1  i have enum lets say for example and have two ...   \n",
       "2  i have extracted the list of sentences from a ...   \n",
       "3  i want to copy some tabular data from excel in...   \n",
       "4  how do you express an integer as a binary numb...   \n",
       "\n",
       "                                        Tags_cleaned  \\\n",
       "0              java objective-c cocoa macos porting    \n",
       "1   c# .net winforms windows-forms-designer prope...   \n",
       "2   python python-2.7 dictionary nltk text-segmen...   \n",
       "3              python arrays list clipboard pywin32    \n",
       "4             python syntax binary integer literals    \n",
       "\n",
       "                                     Title_tokenized  \\\n",
       "0  [porting, java, code, objc, best, represent, c...   \n",
       "1  [way, hiding, enum, values, specific, property...   \n",
       "2  [fixing, words, spaces, using, dictionary, loo...   \n",
       "3              [split, string, line, breaks, python]   \n",
       "4                [express, binary, literals, python]   \n",
       "\n",
       "                                      Body_tokenized  \\\n",
       "0  [working, porting, java, codebase, cocoaobject...   \n",
       "1  [enum, lets, say, example, two, classes, prope...   \n",
       "2  [extracted, list, sentences, document, preproc...   \n",
       "3  [want, copy, tabular, data, excel, python, arr...   \n",
       "4  [express, integer, binary, number, python, lit...   \n",
       "\n",
       "                                      Tags_tokenized  \\\n",
       "0         [java, objective-c, cocoa, macos, porting]   \n",
       "1  [c#, .net, winforms, windows-forms-designer, p...   \n",
       "2  [python, python-2.7, dictionary, nltk, text-se...   \n",
       "3         [python, arrays, list, clipboard, pywin32]   \n",
       "4        [python, syntax, binary, integer, literals]   \n",
       "\n",
       "                    Tags_list  number_of_tags        Id  Score  ViewCount  \\\n",
       "0  [java, objective-c, macos]               3   1117384      8       1346   \n",
       "1                  [c#, .net]               2  59024032      8       2172   \n",
       "2                    [python]               1  19675106     19       6507   \n",
       "3            [python, arrays]               2  21205074      6       7848   \n",
       "4                    [python]               1      1476    394     317942   \n",
       "\n",
       "   AnswerCount  \n",
       "0            6  \n",
       "1            1  \n",
       "2            7  \n",
       "3            5  \n",
       "4            8  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concaténation de title + body\n",
    "df['text'] = [list(x.tolist()+ y.tolist()) for x, y in zip(df['Title_tokenized'], df['Body_tokenized'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['text_sentences'] = df['Title_cleaned']+' '+ df['Body_cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Id', inplace=True)\n",
    "X = df[\"text\"]\n",
    "y = df[\"Tags_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i want to copy some tabular data from excel into a python array that is user willselect a range in an excel table press copy ctrl+c so that the range will be copied to clipboard then i will get this clipboard data into a python array list i use to get clipboard data into an array i copy the following range from excel when i use the function above i get a string like how to split this string into a list so that the list will look like i use method but it doesn't give me what i want \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Body_cleaned'][21205074]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y: (43801, 50)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Multilabel binarizer for targets\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit(y)\n",
    "y_binarized = multilabel_binarizer.transform(y)\n",
    "\n",
    "print(\"Shape of y: {}\".format(y_binarized.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons tester plusieurs métriques sur ce modèle LDA :\n",
    "\n",
    "- Accuracy score :\n",
    "- F1 score :\n",
    "- Jaccard similarity score :\n",
    "- Recall :\n",
    "- Precision :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def metrics_score(model, df, y_true, y_pred):\n",
    "    \"\"\"Compilation function of metrics specific to multi-label\n",
    "    classification problems in a Pandas DataFrame.\n",
    "    This dataFrame will have 1 row per metric\n",
    "    and 1 column per model tested. \n",
    "\n",
    "    Parameters\n",
    "    ----------------------------------------\n",
    "    model : string\n",
    "        Name of the tested model\n",
    "    df : DataFrame \n",
    "        DataFrame to extend. \n",
    "        If None : Create DataFrame.\n",
    "    y_true : array\n",
    "        Array of true values to test\n",
    "    y_pred : array\n",
    "        Array of predicted values to test\n",
    "    ----------------------------------------\n",
    "    \"\"\"\n",
    "    if(df is not None):\n",
    "        temp_df = df\n",
    "    else:\n",
    "        temp_df = pd.DataFrame(index=[\"Accuracy\", \"F1\",\n",
    "                                      \"Jaccard\", \"Recall\",\n",
    "                                      \"Precision\"],\n",
    "                               columns=[model])\n",
    "        \n",
    "    scores = []\n",
    "    scores.append(metrics.accuracy_score(y_true, y_pred))\n",
    "    scores.append(metrics.f1_score(y_pred, \n",
    "                                   y_true, \n",
    "                                   average='weighted'))\n",
    "    scores.append(metrics.jaccard_score(y_true, \n",
    "                                        y_pred, \n",
    "                                        average='weighted'))\n",
    "    scores.append(metrics.recall_score(y_true, \n",
    "                                       y_pred, \n",
    "                                       average='weighted'))\n",
    "    scores.append(metrics.precision_score(y_true, \n",
    "                                          y_pred, \n",
    "                                          average='weighted'))\n",
    "    temp_df[model] = scores\n",
    "    \n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" TFIDF for Doc\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\",\n",
    "                             max_df=.6,\n",
    "                             min_df=0.005,\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=' '.join,\n",
    "                             stop_words=None,\n",
    "                             lowercase=False)\n",
    "\n",
    "vectorizer.fit(X)\n",
    "X_tfidf = vectorizer.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape : (30660, 1573)\n",
      "X_test shape : (13141, 1573)\n",
      "y_train shape : (30660, 50)\n",
      "y_test shape : (13141, 50)\n"
     ]
    }
   ],
   "source": [
    "# Create train and test split (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_binarized,\n",
    "                                                    test_size=0.3, random_state=8)\n",
    "print(\"X_train shape : {}\".format(X_train.shape))\n",
    "print(\"X_test shape : {}\".format(X_test.shape))\n",
    "print(\"y_train shape : {}\".format(y_train.shape))\n",
    "print(\"y_test shape : {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def tokenizer_fct(sentence) :\n",
    "    # print(sentence)\n",
    "    sentence_clean = sentence.replace('-', ' ').replace('+', ' ').replace('/', ' ').replace('#', ' ')\n",
    "    word_tokens = word_tokenize(sentence_clean)\n",
    "    return word_tokens\n",
    "\n",
    "# Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_w = list(set(stopwords.words('english'))) + ['[', ']', ',', '.', ':', '?', '(', ')']\n",
    "\n",
    "def stop_word_filter_fct(list_words) :\n",
    "    filtered_w = [w for w in list_words if not w in stop_w]\n",
    "    filtered_w2 = [w for w in filtered_w if len(w) > 2]\n",
    "    return filtered_w2\n",
    "\n",
    "# lower case et alpha\n",
    "def lower_start_fct(list_words) :\n",
    "    lw = [w.lower() for w in list_words if (not w.startswith(\"@\")) \n",
    "    #                                   and (not w.startswith(\"#\"))\n",
    "                                       and (not w.startswith(\"http\"))]\n",
    "    return lw\n",
    "\n",
    "# Lemmatizer (base d'un mot)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemma_fct(list_words) :\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_w = [lemmatizer.lemmatize(w) for w in list_words]\n",
    "    return lem_w\n",
    "\n",
    "# Fonction de préparation du texte pour le bag of words (Countvectorizer et Tf_idf, Word2Vec)\n",
    "def transform_bow_fct(desc_text) :\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(sw)\n",
    "    # lem_w = lemma_fct(lw)    \n",
    "    transf_desc_text = ' '.join(lw)\n",
    "    return transf_desc_text\n",
    "\n",
    "# Fonction de préparation du texte pour le bag of words avec lemmatization\n",
    "def transform_bow_lem_fct(desc_text) :\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(sw)\n",
    "    lem_w = lemma_fct(lw)    \n",
    "    transf_desc_text = ' '.join(lem_w)\n",
    "    return transf_desc_text\n",
    "\n",
    "# Fonction de préparation du texte pour le Deep learning (USE et BERT)\n",
    "def transform_dl_fct(desc_text) :\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "#    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(word_tokens)\n",
    "    # lem_w = lemma_fct(lw)    \n",
    "    transf_desc_text = ' '.join(lw)\n",
    "    return transf_desc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Twins\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence_bow'] = df['text_sentences'].apply(lambda x : transform_bow_fct(x))\n",
    "df['sentence_bow_lem'] = df['text_sentences'].apply(lambda x : transform_bow_lem_fct(x))\n",
    "df['sentence_dl'] = df['text_sentences'].apply(lambda x : transform_dl_fct(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  best modèle avec SBERT transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SBERT (Sentence-BERT) est une implémentation spécifique de modèles de Sentence Transformers qui utilise BERT comme composant d'encodage de phrases. Contrairement à BERT qui a été conçu pour traiter des séquences de mots entières, SBERT est conçu pour encoder des phrases entières pour produire des embeddings de phrases sémantiquement similaires.\n",
    "\n",
    "SBERT utilise une méthode d'entraînement appelée \"siamese and triplet network\" pour apprendre des embeddings sémantiques de phrases en utilisant des paires ou des triplets de phrases étiquetées. Le réseau siamois prend en entrée deux phrases et produit des vecteurs d'embeddings de phrases qui sont comparés pour évaluer leur similarité. Le réseau triplet prend en entrée trois phrases (une ancre, une phrase positive et une phrase négative) et utilise la distance entre les embeddings pour maximiser la similarité entre l'ancre et la phrase positive, tout en minimisant la similarité entre l'ancre et la phrase négative.\n",
    "\n",
    "SBERT est souvent utilisé pour des tâches telles que la recherche de phrases similaires, la classification de textes et le résumé automatique. En utilisant SBERT, il est possible d'obtenir des représentations vectorielles de haute qualité pour les phrases qui capturent leur signification et leur contexte sémantique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "# Bert\n",
    "import os\n",
    "import transformers\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file C:\\Users\\Twins/.cache\\torch\\sentence_transformers\\sentence-transformers_all-MiniLM-L6-v2\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"C:\\\\Users\\\\Twins/.cache\\\\torch\\\\sentence_transformers\\\\sentence-transformers_all-MiniLM-L6-v2\\\\\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file C:\\Users\\Twins/.cache\\torch\\sentence_transformers\\sentence-transformers_all-MiniLM-L6-v2\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at C:\\Users\\Twins/.cache\\torch\\sentence_transformers\\sentence-transformers_all-MiniLM-L6-v2\\.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "model_bert = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pour de raison de mémoire, la fonction suivante construit la matrice des features de Bert morceau par morceau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['sentence_dl'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = model_bert.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Twins\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneVsRestClassifier(estimator=LogisticRegression(C=10, penalty=&#x27;l1&#x27;,\n",
       "                                                 random_state=9,\n",
       "                                                 solver=&#x27;saga&#x27;))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneVsRestClassifier</label><div class=\"sk-toggleable__content\"><pre>OneVsRestClassifier(estimator=LogisticRegression(C=10, penalty=&#x27;l1&#x27;,\n",
       "                                                 random_state=9,\n",
       "                                                 solver=&#x27;saga&#x27;))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=10, penalty=&#x27;l1&#x27;, random_state=9, solver=&#x27;saga&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=10, penalty=&#x27;l1&#x27;, random_state=9, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=10, penalty='l1',\n",
       "                                                 random_state=9,\n",
       "                                                 solver='saga'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split features\n",
    "Bertfet_train, Bertfet_test, y_train, y_test = train_test_split(sentence_embeddings, y_binarized, test_size=0.30)\n",
    "reg_logit_clf = OneVsRestClassifier(LogisticRegression(C= 10, penalty= 'l1', solver= 'saga', random_state = 9))\n",
    "reg_logit_clf.fit(Bertfet_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "bert_pred_lr = reg_logit_clf.predict(Bertfet_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13141, 384)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bertfet_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13141, 50)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logitR_BERT</th>\n",
       "      <th>logitR_USE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.327824</td>\n",
       "      <td>0.316210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.670116</td>\n",
       "      <td>0.652936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jaccard</th>\n",
       "      <td>0.496880</td>\n",
       "      <td>0.475392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.587398</td>\n",
       "      <td>0.566866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.733199</td>\n",
       "      <td>0.717987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           logitR_BERT  logitR_USE\n",
       "Accuracy      0.327824    0.316210\n",
       "F1            0.670116    0.652936\n",
       "Jaccard       0.496880    0.475392\n",
       "Recall        0.587398    0.566866\n",
       "Precision     0.733199    0.717987"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics_compare = metrics_score(\"logitR_BERT\", df=df_metrics_compare,\n",
    "                                   y_true=y_test,\n",
    "                                   y_pred=lr_pred_use)\n",
    "df_metrics_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logitR_BERT</th>\n",
       "      <th>logitR_USE</th>\n",
       "      <th>logitR_BERT_ACP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.327824</td>\n",
       "      <td>0.316210</td>\n",
       "      <td>0.294494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.670116</td>\n",
       "      <td>0.652936</td>\n",
       "      <td>0.646140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jaccard</th>\n",
       "      <td>0.496880</td>\n",
       "      <td>0.475392</td>\n",
       "      <td>0.453536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.587398</td>\n",
       "      <td>0.566866</td>\n",
       "      <td>0.530658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.733199</td>\n",
       "      <td>0.717987</td>\n",
       "      <td>0.718362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           logitR_BERT  logitR_USE  logitR_BERT_ACP\n",
       "Accuracy      0.327824    0.316210         0.294494\n",
       "F1            0.670116    0.652936         0.646140\n",
       "Jaccard       0.496880    0.475392         0.453536\n",
       "Recall        0.587398    0.566866         0.530658\n",
       "Precision     0.733199    0.717987         0.718362"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics_compare = metrics_score(\"logitR_BERT_ACP\", df=df_metrics_compare,\n",
    "                                   y_true=y_test,\n",
    "                                   y_pred=bert_pred_lr_PCA)\n",
    "df_metrics_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# selection du modèle et sauvegarde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le modèle choisis : logistic regression + sentence BERT transformer ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = reg_logit_clf # using sentence_transformer - logisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export fitted model and Preprocessor\n",
    "from pickle import dump\n",
    "\n",
    "#Modèle serialisation\n",
    "dump(final_model, open(\"final_model.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(multilabel_binarizer, open(\"multilabel_binarizer.pkl\",\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
